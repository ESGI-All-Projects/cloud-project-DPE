{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Nous allons réaliser une exploration des données, comprenant des analyses univariées et bivariées ainsi que des tests statistiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51114086-25f5-4c9c-8bb3-64ff28b0f0f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###  I. Importing Data and Cleaning Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c51aebd-ee46-48bc-a767-b503273c0959",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data = spark.read.csv(\"/mnt/data/train.csv\", header=True, inferSchema=True)\n",
    "test_data = spark.read.csv(\"/mnt/data/test.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Combining train and test datasets\n",
    "# We combine the two datasets to address the undersampling of label 'A'. The union operation merges the rows of both datasets.\n",
    "data = train_data.union(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec0eb47-1220-48a6-81b4-adb261a7265a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(data.select(\"Hauteur_sous-plafond\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bed9f143-6888-477d-a844-e59134e81fc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Removing outliers from the dataset\n",
    "data = data.filter(F.col(\"Surface_habitable_logement\") <= 100000)\n",
    "data = data.filter(F.col(\"Conso_5_usages/m²_é_finale\") <= 100000)\n",
    "data = data.filter(F.col(\"Hauteur_sous-plafond\") <= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "172631f9-57cb-452a-a70e-bdc24c77e9c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92108b48-e8f5-4d0d-9d24-3775b36ca207",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## II. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfef454a-6e4c-4c66-abfb-d8d4bd81481e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Splitting the dataset into numerical and categorical variables\n",
    "\n",
    "# Selecting numerical variables\n",
    "# We iterate through the DataFrame's columns and select those with data types indicating numerical values.\n",
    "numerical_data = data.select([col(c) for c, t in data.dtypes if t in ['int', 'bigint', 'float', 'double']])\n",
    "#numerical_vars\n",
    "\n",
    "# Selecting categorical (qualitative) variables\n",
    "# Similarly, we select columns where the data type is 'string', indicating categorical variables.\n",
    "categorical_data = data.select([col(c) for c, t in data.dtypes if t == 'string'])\n",
    "#categorical_vars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50e119b-f4cf-4acf-bba0-a55218ea7471",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 1. Processing Quantitative Values\n",
    "\n",
    "Removing variables/columns who have more than 10% NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ebdc5e6-b609-4990-b610-d411cb3ccce3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessary functions from PySpark\n",
    "from pyspark.sql.functions import col, mean, count, when\n",
    "\n",
    "# Calculating the percentage of missing (null) values for each numerical column\n",
    "missing_value_stats = (numerical_data.select(*[\n",
    "    (count(when(col(column).isNull(), column)) / count(\"*\")).alias(column) \n",
    "    for column in numerical_data.columns\n",
    "]).toPandas().transpose() * 100).round(4)\n",
    "\n",
    "# Displaying the percentage of missing values for each column\n",
    "print(missing_value_stats.sort_values(by=0))\n",
    "\n",
    "# Columns with more than 10% missing values are identified for removal.\n",
    "columns_to_remove = missing_value_stats.index[missing_value_stats[0] > 10].tolist()\n",
    "\n",
    "data = data.drop(*columns_to_remove)\n",
    "numerical_data = numerical_data.drop(*columns_to_remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74cd4f0d-79c4-4a6e-b458-356c534b2af6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# These columns are considered irrelevant or redundant, such as duplicate postal code columns.\n",
    "columns_to_discard = [\"_c0\", \"Code_postal_(BAN)\", \"Conso_5_usages_é_finale\", \"Code_postal_(brut)\", \"Emission_GES_éclairage\"]\n",
    "\n",
    "# Dropping specified columns from the dataset\n",
    "# Both the main dataset 'data' and the subset 'numerical_data' are updated by removing these columns.\n",
    "data = data.drop(*columns_to_discard)\n",
    "numerical_data = numerical_data.drop(*columns_to_discard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39849f2e-5184-4cdc-a4a6-4b1ad46c29c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col, when\n",
    "\n",
    "# Replacing missing values with the mean for each column in numerical_data\n",
    "# We iterate over each column and calculate its mean, then replace null values with this mean.\n",
    "for col_name in numerical_data.columns:\n",
    "    mean_val = data.agg(avg(col(col_name)).alias('mean')).first()['mean']\n",
    "    data = data.withColumn(col_name, when(col(col_name).isNull(), mean_val).otherwise(col(col_name)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1b58635-1ff6-4cbc-8c68-f714dc837aa9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Processing Qualitative Values\n",
    "\n",
    "Removing variables who have more than 10% NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fe0bbf3-668d-4b90-9e38-465d370fca21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# Calculating the percentage of missing (null) values for each categorical column\n",
    "missing_value_stats_cat = (categorical_data.select(*[\n",
    "    (count(when(col(column).isNull(), column)) / count(\"*\")).alias(column) \n",
    "    for column in categorical_data.columns\n",
    "]).toPandas().transpose() * 100).round(4) \n",
    "\n",
    "# Displaying the percentage of missing values for each column\n",
    "print(missing_value_stats_cat.sort_values(by=0))\n",
    "\n",
    "# Identifying columns with more than 10% missing values to be removed\n",
    "# Columns exceeding the threshold are marked for removal.\n",
    "columns_to_remove_cat = missing_value_stats_cat.index[missing_value_stats_cat[0] > 10].tolist()\n",
    "\n",
    "data = data.drop(*columns_to_remove_cat)\n",
    "categorical_data = categorical_data.drop(*columns_to_remove_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "525af0ba-79f8-45a1-9cdc-e544f09ae7ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# These columns are deemed unnecessary, either because they contain unique values or are not informative.\n",
    "columns_to_discard_cat = [\"N°DPE\", \"Code_INSEE_(BAN)\", \"Qualité_isolation_plancher_bas\", \"Nom__commune_(Brut)\"]\n",
    "\n",
    "# The operation is applied to both 'data' and 'categorical_data' DataFrames.\n",
    "data = data.drop(*columns_to_discard_cat)\n",
    "categorical_data = categorical_data.drop(*columns_to_discard_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d746d19-428a-4433-9ae2-dd5a906ee84e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Replacing missing values with 0 in categorical columns\n",
    "for col_name in categorical_data.columns:\n",
    "    data = data.withColumn(col_name, when(col(col_name).isNull(), 0).otherwise(col(col_name)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae1944a2-513f-4c2b-9c51-6fda339194b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 3. Converting Quantitative Variables into Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd32b1bf-b0e9-4fef-8ed2-f5d9a9755372",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Surface habitable logement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b238f30-f95c-42e3-a92b-86d3621cf68a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Defining the number of categories (e.g., 4 for quartiles)\n",
    "num_categories = 4\n",
    "# Calculating the quantiles for the variable\n",
    "quantile_values = data.approxQuantile(\"Surface_habitable_logement\", [0.25, 0.5, 0.75], 0.05)\n",
    "\n",
    "# Assigning categories based on quantile thresholds\n",
    "data = data.withColumn(\"Surface_habitable_logement_cat\",\n",
    "                       when(data[\"Surface_habitable_logement\"] <= quantile_values[0], \"Q1\")\n",
    "                       .when(data[\"Surface_habitable_logement\"] <= quantile_values[1], \"Q2\")\n",
    "                       .when(data[\"Surface_habitable_logement\"] <= quantile_values[2], \"Q3\")\n",
    "                       .otherwise(\"Q4\"))\n",
    "\n",
    "# Displaying the first few rows to check the result\n",
    "data.select(\"Surface_habitable_logement\", \"Surface_habitable_logement_cat\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7572a371-7ebf-49da-9a60-4ed2fdb09004",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Counting the occurrences of each category in the 'Surface_habitable_logement_cat' column\n",
    "category_counts = data.groupBy(\"Surface_habitable_logement_cat\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# This gives an overview of the distribution of data across the defined categories.\n",
    "category_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd442855-f081-4751-b7da-3c21d914d332",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Counting and sorting the occurrences of each unique value in 'Surface_habitable_logement'\n",
    "sorted_value_counts = (data.groupBy(\"Surface_habitable_logement\")\n",
    "                       .agg(count(\"*\").alias(\"count\"))\n",
    "                       .orderBy(\"Surface_habitable_logement\", ascending=False))\n",
    "\n",
    "# This will show the frequency of each unique value in the 'Surface_habitable_logement' column, sorted from the highest to lowest.\n",
    "sorted_value_counts.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52bdb551-8082-4470-a6bc-9261f5140a9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Consommation 5 usages / m²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "725970c6-33eb-47b1-a055-91a15ffa8094",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "histogram_data = data.select(\"Conso_5_usages/m²_é_finale\").rdd.flatMap(lambda x: x).histogram(20)\n",
    "bins = histogram_data[0]\n",
    "freqs = histogram_data[1]\n",
    "plt.hist(bins[:-1], bins=bins, weights=freqs)\n",
    "plt.show()\n",
    "\n",
    "# on voit bien qu'il y a encore des varibales abérantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d0acbed-2dc8-4b23-9676-234398951512",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Defining the number of categories, such as 4 for quartiles\n",
    "num_quartiles = 4\n",
    "\n",
    "# Calculating the quartiles for the specific column\n",
    "# The 'approxQuantile' function is used to find the quartile thresholds with a relative error of 5%.\n",
    "quartile_thresholds = data.approxQuantile(\"Conso_5_usages/m²_é_finale\", [0.25, 0.5, 0.75], 0.05)\n",
    "\n",
    "# Categorizing the data based on the quartile thresholds\n",
    "data = data.withColumn(\"Conso_5_usages_cat\",\n",
    "                       when(col(\"Conso_5_usages/m²_é_finale\") <= quartile_thresholds[0], \"Q1\")\n",
    "                       .when(col(\"Conso_5_usages/m²_é_finale\") <= quartile_thresholds[1], \"Q2\")\n",
    "                       .when(col(\"Conso_5_usages/m²_é_finale\") <= quartile_thresholds[2], \"Q3\")\n",
    "                       .otherwise(\"Q4\"))\n",
    "\n",
    "# This step helps in checking the categorization based on the computed quartiles.\n",
    "data.select(\"Conso_5_usages/m²_é_finale\", \"Conso_5_usages_cat\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5216dea-31d2-402c-8350-84b829936fa6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Counting the occurrences of each category in 'Conso_5_usages_cat' column\n",
    "category_counts = data.groupBy(\"Conso_5_usages_cat\").agg(count(\"*\").alias(\"category_count\"))\n",
    "\n",
    "# This provides an overview of how many entries fall into each category.\n",
    "category_counts.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6be11ad4-a49d-4c91-b0dc-b0dce37a5101",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Hauteur sous-plafond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88c648d7-fb43-45dd-ae60-87f22635d601",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "histogram_data = data.select(\"Hauteur_sous-plafond\").rdd.flatMap(lambda x: x).histogram(20)\n",
    "bins = histogram_data[0]\n",
    "freqs = histogram_data[1]\n",
    "plt.hist(bins[:-1], bins=bins, weights=freqs)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d77653ec-d910-4014-86e9-e7ddd929ea07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Manually specifying the bin ranges\n",
    "bins = [0, 2.2, 2.4, 2.5, 3, 6, 10]  \n",
    "labels = ['0-2.2', '2.2-2.4', '2.4-2.5', '2.5-3', '3-6', '6-10']  \n",
    "\n",
    "# Creating a new column 'Hauteur_category' with categorized values\n",
    "# The 'when' function is used to assign each value to a category based on the specified bins.\n",
    "data = data.withColumn('Hauteur_category', \n",
    "                       when((data['Hauteur_sous-plafond'] > bins[0]) & (data['Hauteur_sous-plafond'] <= bins[1]), labels[0])\n",
    "                       .when((data['Hauteur_sous-plafond'] > bins[1]) & (data['Hauteur_sous-plafond'] <= bins[2]), labels[1])\n",
    "                       .when((data['Hauteur_sous-plafond'] > bins[2]) & (data['Hauteur_sous-plafond'] <= bins[3]), labels[2])\n",
    "                       .when((data['Hauteur_sous-plafond'] > bins[3]) & (data['Hauteur_sous-plafond'] <= bins[4]), labels[3])\n",
    "                       .when((data['Hauteur_sous-plafond'] > bins[4]) & (data['Hauteur_sous-plafond'] <= bins[5]), labels[4])\n",
    "                       .when(data['Hauteur_sous-plafond'] > bins[5], labels[5])\n",
    "                       .otherwise(\"Other\"))\n",
    "\n",
    "# This step is useful for checking if the categorization has been applied correctly.\n",
    "data.select('Hauteur_sous-plafond', 'Hauteur_category').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30e9916d-8eba-4829-a654-62bcba2866dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Counting the occurrences of each category in 'Hauteur_categorie' column\n",
    "category_value_counts = data.groupBy(\"Hauteur_category\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# This shows how many records are in each category of 'Hauteur_categorie'.\n",
    "category_value_counts.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8abe7239-0632-4b90-baf0-38549b038a38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## IV. Univariate plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c99b42c8-ce3c-445b-a4ec-a4a6c51172b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_data.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f8ef6f-578a-419f-b488-8c6510359811",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "top_n = 10\n",
    "\n",
    "for var in categorical_data.columns:\n",
    "    top_modalities = data.groupBy(var).count().orderBy('count', ascending=False).limit(top_n).select(var).rdd.flatMap(lambda x: x).collect()\n",
    "    data = data.withColumn(var + \"_top_n\", when(col(var).isin(top_modalities), col(var)).otherwise(lit('Autres')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e44091-5900-40d3-ae25-cdeea1ab787b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure the number of rows and columns based on the number of categorical variables\n",
    "n_vars = len(categorical_data.columns)\n",
    "n_cols = 2\n",
    "n_rows = (n_vars + n_cols - 1) // n_cols  # Ceiling division to get enough rows\n",
    "\n",
    "# Adjust the figure size dynamically based on the number of rows\n",
    "plt.figure(figsize=(20, 5 * n_rows))\n",
    "\n",
    "for i, var in enumerate(categorical_data.columns):\n",
    "    ax = plt.subplot(n_rows, n_cols, i + 1)\n",
    "    data_pd = data.groupBy(var + \"_top_n\").count().orderBy('count', ascending=False).toPandas()\n",
    "    \n",
    "    sns.barplot(x='count', y=var + \"_top_n\", data=data_pd, palette='viridis')\n",
    "    plt.title(f'Distribution of {var}', fontsize=14)\n",
    "    \n",
    "    ax.set_yticklabels([label.get_text()[:30] + '...' if len(label.get_text()) > 30 else label.get_text() for label in ax.get_yticklabels()], fontsize=10)\n",
    "    \n",
    "    # Formatting the count labels to include thousand separator and avoid overlap by adjusting text position\n",
    "    for p in ax.patches:\n",
    "        width = p.get_width()\n",
    "        plt.text(width, p.get_y() + p.get_height() / 2, f'{int(width):,}', ha='left', va='center', fontsize=10)\n",
    "\n",
    "# Adjust the layout\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80efbcd-0b1e-4679-9970-be4159651fe3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_vars = len(numerical_data.columns)\n",
    "n_cols = 2\n",
    "n_rows = (n_vars + n_cols - 1) // n_cols  \n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 4 * n_vars))\n",
    "\n",
    "\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for i, var in enumerate(numerical_data.columns):\n",
    "    bins, counts = data.select(var).rdd.flatMap(lambda x: [x[0]] if x[0] is not None else []).histogram(20)\n",
    "    ax = axes_flat[i]\n",
    "    ax.hist(bins[:-1], bins=bins, weights=counts, edgecolor='black', alpha=0.7, log=True)\n",
    "    for count, bin in zip(counts, bins[:-1]):\n",
    "        if count > 0:\n",
    "            ax.text(bin + (bins[1] - bins[0])/2, count, f'{int(count)}',\n",
    "                    va='bottom', ha='center', fontsize=8)\n",
    "\n",
    "    ax.set_title(f'Distribution of {var}')\n",
    "    ax.set_xlabel(var)\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95f5d3b1-97ec-465c-bf5d-95795a28738f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## V. Bivariate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73f849fc-f3de-4145-badc-f5ab01156354",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit, expr\n",
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_stacked_bar(data, variable_analyse, variable_cible, top_n=10):\n",
    "    top_modalities = data.groupBy(variable_analyse).count().orderBy('count', ascending=False).limit(top_n).select(variable_analyse).rdd.flatMap(lambda x: x).collect()\n",
    "    data_top_n = data.withColumn(variable_analyse, when(col(variable_analyse).isin(top_modalities), col(variable_analyse)).otherwise(lit('Autres')))\n",
    "    pivot_data = data_top_n.groupBy(variable_analyse).pivot(variable_cible).count()\n",
    "    pivot_data = pivot_data.fillna(0)\n",
    "\n",
    "    columns_to_sum = [col(c) for c in pivot_data.columns if c != variable_analyse]\n",
    "\n",
    "    pivot_data = pivot_data.select(\"*\", expr(\"A + B + C + D + E + F + G as total\"))\n",
    "\n",
    "    for c in pivot_data.columns:\n",
    "        if c != variable_analyse and c != 'total':\n",
    "            pivot_data = pivot_data.withColumn(c, (col(c) / col('total')) * 100)       \n",
    "    pivot_percent_pd = pivot_data.toPandas().set_index(variable_analyse)\n",
    "    ax = pivot_percent_pd.drop('total', axis=1).plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "    \n",
    "    plt.title(f'Distribution de {variable_analyse} par {variable_cible} (en %)')\n",
    "    plt.xlabel(f'{variable_analyse}')\n",
    "    plt.ylabel('Pourcentage')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.legend(title=f'{variable_cible}', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')\n",
    "\n",
    "    for p in ax.patches:\n",
    "        width = p.get_width()\n",
    "        height = p.get_height()\n",
    "        x, y = p.get_xy() \n",
    "        if height > 3:\n",
    "            ax.text(x + width/2, y + height/2, '{:1.1f}%'.format(height), ha='center', va='center', fontsize=8)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39b81276-1306-4367-8177-c146cb610146",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    " Etiquette GES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5854edf4-bc07-4ee3-8c68-1ef85b9f526c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_stacked_bar(data, \"Etiquette_GES\", \"Etiquette_DPE\", top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e1a7333-21fc-4cdd-b205-48df0e051e72",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    " Classe Altitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d8a4b4d-9fb6-4a98-ae4e-8dabbbccec44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_stacked_bar(data, \"Classe_altitude\", \"Etiquette_DPE\", top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7342b5df-0367-4dd3-b449-cb19a72bb5ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    " Département"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f77f0710-c466-4b3b-9a22-7def8c9b7a50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_stacked_bar(data, \"N°_département_(BAN)\", \"Etiquette_DPE\", top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26b0f988-64c5-4426-b426-e4ca82c88b13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    " Qualité isolation enveloppe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "287c8a2a-ab3d-43a5-b8bf-0cfbb9c0cfdf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_stacked_bar(data, \"Qualité_isolation_enveloppe\", \"Etiquette_DPE\", top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a836802e-4245-459d-8e05-24b4857d0442",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    " Qualité isolation murs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f118004-0884-47bd-8495-537e6cedcede",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_stacked_bar(data, \"Qualité_isolation_murs\", \"Etiquette_DPE\", top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0d595de-7894-4bbc-a2e6-4cc54c3a5a05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    " Qualité isolation menuiseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a219ec8f-a3a9-4ff2-a937-d48edb440df4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_stacked_bar(data, \"Qualité_isolation_menuiseries\", \"Etiquette_DPE\", top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fc3c430-c414-4d0d-9362-3dc9bb567ed2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    " Type bâtiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "397e9685-eb72-4756-806b-9e322cc5609c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_stacked_bar(data, \"Type_bâtiment\", \"Etiquette_DPE\", top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98c92c89-8dde-4831-9e38-0bba99778b4d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    " Surface logement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77d5ecb2-6992-4c6f-a252-65d16a746a8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_stacked_bar(data, \"Surface_habitable_logement_cat\", \"Etiquette_DPE\", top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf047728-976f-4acb-9772-2dfb730b74c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    " Conso 5 usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "146287db-8f7c-4113-85ee-3665d3145acc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_stacked_bar(data, \"Conso_5_usages_cat\", \"Etiquette_DPE\", top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6caeaf4-71c4-4e1e-a344-028f4beaaf91",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 6. Tests statistiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9da2eb39-b4e2-46c9-b21c-dc1585c02d02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "variable_cible = \"Etiquette_DPE\"\n",
    "\n",
    "def compute_chi2_contingency(data, var, variable_cible):\n",
    "    # Créer un tableau de contingence\n",
    "    contingency_table = data.crosstab(var, variable_cible)\n",
    "    \n",
    "    # Convertir en format approprié pour chi2_contingency\n",
    "    contingency_table_pd = contingency_table.toPandas()\n",
    "    contingency_table_pd = contingency_table_pd.set_index(contingency_table_pd.columns[0])\n",
    "    contingency_table_values = contingency_table_pd.values\n",
    "\n",
    "    # Calculer le test du chi-deux\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table_values)\n",
    "    return chi2, p, dof, expected\n",
    "\n",
    "def cramers_v(chi2, n, k1, k2):\n",
    "    \"\"\"Calcul du V de Cramer.\"\"\"\n",
    "    return np.sqrt(chi2 / (n * min(k1 - 1, k2 - 1)))\n",
    "\n",
    "# Initialisation des listes pour stocker les résultats\n",
    "variables = []\n",
    "cramers_v_values = []\n",
    "\n",
    "# Calculer le V de Cramer pour chaque variable catégorielle par rapport à la variable cible\n",
    "for var in categorical_data.columns:\n",
    "    chi2, p, dof, expected = compute_chi2_contingency(data, var, variable_cible)\n",
    "    n = np.sum(expected)  # Taille totale de l'échantillon\n",
    "    k1, k2 = expected.shape\n",
    "    cramers_v_value = cramers_v(chi2, n, k1, k2)\n",
    "    print(f\"{var}, p-value = {p}\")\n",
    "    variables.append(var)\n",
    "    cramers_v_values.append(cramers_v_value)\n",
    "\n",
    "# Créer un DataFrame pour les résultats\n",
    "cramers_v_df = pd.DataFrame({'Variable': variables, 'Cramers_V': cramers_v_values})\n",
    "\n",
    "# Trier les données pour une meilleure visualisation\n",
    "cramers_v_df = cramers_v_df.sort_values('Cramers_V', ascending=True)\n",
    "\n",
    "# Créer un graphique à barres horizontales\n",
    "plt.figure(figsize=(10, len(variables) * 0.5))  # Ajuster la taille en fonction du nombre de variables\n",
    "plt.barh(cramers_v_df['Variable'], cramers_v_df['Cramers_V'], color='skyblue')\n",
    "plt.xlabel('Valeur du V de Cramer')\n",
    "plt.title('Force de l\\'association (V de Cramer) par rapport à ' + variable_cible)\n",
    "\n",
    "# Ajouter des étiquettes pour chaque barre\n",
    "for index, value in enumerate(cramers_v_df['Cramers_V']):\n",
    "    plt.text(value, index, f'{value:.2f}')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [
      {
       "dashboardResultIndex": 0,
       "elementNUID": "146287db-8f7c-4113-85ee-3665d3145acc",
       "elementType": "command",
       "guid": "32cd05bb-8d43-429e-b7d0-f0234ce1dc6b",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      }
     ],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "b25a306a-5d8b-4920-af5c-ca5c9e9f6cdc",
     "origId": 4083891481783637,
     "title": "Dashboard",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Explore data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
